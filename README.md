# ğŸŒŸ dbt + Airflow + DuckDB: Mini Star Schema Analytics Project

This is a lightweight, end-to-end data pipeline built with [dbt Core](https://www.getdbt.com/), [Apache Airflow](https://airflow.apache.org/), and [DuckDB](https://duckdb.org/). It demonstrates how to orchestrate and model data in a local analytics engineering environment â€” no cloud required.

## ğŸ”§ Tech Stack

- **Airflow**: DAG orchestration (daily run)
- **dbt Core**: SQL-based transformations and testing
- **DuckDB**: In-process SQL analytics engine (local file storage)
- **Mockaroo**: Seed data (customers, products, orders)

## ğŸ§± Data Model

The pipeline builds a star schema with:

### ğŸ“¦ Fact Table
- `fct_orders`: Order-level data, joined with products and calculated total sales

### ğŸ§‘â€ğŸ¤â€ğŸ§‘ Dimension Tables
- `dim_customers`: Full name, email, signup region/year
- `dim_products`: Product metadata with category and price

### ğŸ”„ Staging Models
- Raw seed cleanup and typing

## ğŸ“ˆ Example Metrics (could be extended)
- Total sales by category
- Customers by region
- Repeat customer rate
- Lifetime value (LTV)

## âœ… dbt Features Used

- `seeds`, `ref`, `sources`
- `tests`: unique + not null
- `Jinja` + `strptime()` for DuckDB compatibility
- `schema.yml` documentation
- Materializations: `view`, `table`

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€dbt_duckdb_dag.py           
â”œâ”€â”€ dbt_duckdb_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_orders.sql
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_products.sql
â”‚   â”‚   â”‚   â””â”€â”€ fct_orders.sql
â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â””â”€â”€ orders.csv
```

### ğŸ›  Airflow DAG

This project includes an [Apache Airflow](https://airflow.apache.org/) DAG that orchestrates the full dbt workflow locally. The DAG includes the following steps:

1. **Install dependencies** with `dbt deps`
2. **Seed data** from local CSVs with `dbt seed`
3. **Build models** using `dbt run`
4. **Run tests** to validate models with `dbt test`

The DAG is manually triggerable (no schedule by default) and is designed to run against a local DuckDB target. It showcases orchestration skills and simulates a production-style data pipeline using open-source tools only.

DAG file path:
```
airflow/dags/dbt_duckdb_pipeline.py
```

---

## ğŸš€ How to Run

1. Clone the repo
2. Install dependencies (`pip install dbt-duckdb`)
3. Run:
   ```
   dbt seed
   dbt build
   dbt docs serve
   ```

Optional: Airflow DAGs can be used to run this on a schedule.

---

## ğŸ™‹â€â™€ï¸ About Me

I'm a full-stack data professional with experience in cloud data warehousing, ELT pipelines, and BI tooling. This project showcases my skills across modeling, orchestration, testing, and warehouse design â€” all using open-source, local-first tools.

ğŸ‘‰ [Connect on LinkedIn](https://www.linkedin.com/in/melissa-nicholas-7a143593/)


Built with â¤ï¸ by Melissa Nicholas
